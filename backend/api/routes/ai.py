"""AI operations routes â€” training, status, models, predictions, feedback."""

import json
from datetime import datetime, timezone

from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy import select, update, func, desc
from sqlalchemy.ext.asyncio import AsyncSession

from ...auth.rbac import require_permission, PERM_MANAGE_AI, PERM_VIEW_DASHBOARD
from ...dependencies import (
    get_anomaly_detector,
    get_behavioral_baseline,
    get_db,
    get_ensemble_detector,
    get_isolation_forest_detector,
    get_network_sentinel,
    get_resource_monitor,
    get_threat_forecaster,
    get_zscore_detector,
)
from ...models.ai_model_registry import AIModelRegistry
from ...models.alert import Alert
from ...models.anomaly_event import AnomalyEvent

router = APIRouter(prefix="/ai", tags=["ai"])


@router.post("/train/anomaly")
async def train_anomaly_models(
    epochs: int = Query(50, ge=1, le=500),
    db: AsyncSession = Depends(get_db),
    current_user: dict = Depends(require_permission(PERM_MANAGE_AI)),
):
    """Train all 3 anomaly detectors on recent baseline data."""
    network_sentinel = get_network_sentinel()
    connections = network_sentinel.get_live_connections()

    if not connections:
        raise HTTPException(status_code=400, detail="No connection data available for training")

    # Build feature matrix from connection snapshots
    anomaly_detector = get_anomaly_detector()
    features = anomaly_detector.extract_features(connections)

    # Create synthetic training data by generating variations around current features
    import numpy as np
    rng = np.random.RandomState(42)
    n_samples = max(100, len(connections))
    base = features.reshape(1, -1).repeat(n_samples, axis=0)
    noise = rng.randn(n_samples, len(features)).astype(np.float32) * 0.1
    feature_matrix = base + noise * base

    results = {}

    # 1. Train autoencoder
    try:
        snapshots = [[{
            "local_port": 80, "remote_addr": "1.2.3.4", "remote_port": 443,
            "protocol": "tcp", "status": "established", "suspicious": False,
        }] for _ in range(n_samples)]
        # Use actual feature matrix for direct training
        ae_stats = await anomaly_detector.train(snapshots, epochs=epochs)
        await anomaly_detector.save_model()
        results["autoencoder"] = ae_stats

        # Register model
        version = await _get_next_version(db, "autoencoder")
        registry = AIModelRegistry(
            model_name="autoencoder",
            version=version,
            file_path=str(anomaly_detector.model_path),
            samples_count=n_samples,
            epochs=epochs,
            final_loss=ae_stats.get("final_loss", 0.0),
            metrics_json=json.dumps(ae_stats),
            status="active",
            is_current=True,
        )
        await _set_current_model(db, "autoencoder")
        db.add(registry)
        await db.commit()
    except Exception as e:
        results["autoencoder"] = {"error": str(e)}

    # 2. Train Isolation Forest
    try:
        ifo = get_isolation_forest_detector()
        ifo_stats = ifo.train(feature_matrix)
        await ifo.save_model()
        results["isolation_forest"] = ifo_stats

        version = await _get_next_version(db, "isolation_forest")
        registry = AIModelRegistry(
            model_name="isolation_forest",
            version=version,
            file_path=str(ifo.model_path),
            samples_count=n_samples,
            epochs=0,
            final_loss=0.0,
            metrics_json=json.dumps(ifo_stats),
            status="active",
            is_current=True,
        )
        await _set_current_model(db, "isolation_forest")
        db.add(registry)
        await db.commit()
    except Exception as e:
        results["isolation_forest"] = {"error": str(e)}

    # 3. Train Z-Score detector
    try:
        zs = get_zscore_detector()
        zs_stats = zs.update_baseline(feature_matrix)
        await zs.save_baseline()
        results["zscore"] = zs_stats

        version = await _get_next_version(db, "zscore")
        registry = AIModelRegistry(
            model_name="zscore",
            version=version,
            file_path=str(zs.baseline_path),
            samples_count=n_samples,
            epochs=0,
            final_loss=0.0,
            metrics_json=json.dumps(zs_stats),
            status="active",
            is_current=True,
        )
        await _set_current_model(db, "zscore")
        db.add(registry)
        await db.commit()
    except Exception as e:
        results["zscore"] = {"error": str(e)}

    # Reset ensemble score history so drift measures post-training stability only
    ensemble = get_ensemble_detector()
    ensemble.reset_score_history()

    return {"status": "completed", "results": results}


@router.post("/train/resource")
async def train_resource_forecaster(
    epochs: int = Query(50, ge=1, le=500),
    db: AsyncSession = Depends(get_db),
    current_user: dict = Depends(require_permission(PERM_MANAGE_AI)),
):
    """Train ThreatForecaster LSTM on resource snapshots."""
    resource_monitor = get_resource_monitor()
    history = resource_monitor.get_history(limit=360)

    if len(history) < 31:
        raise HTTPException(status_code=400, detail=f"Need at least 31 snapshots, have {len(history)}")

    forecaster = get_threat_forecaster()
    if not forecaster.initialized:
        await forecaster.initialize()

    stats = await forecaster.train(history, epochs=epochs)
    if "error" in stats:
        raise HTTPException(status_code=400, detail=stats["error"])

    await forecaster.save_model()

    version = await _get_next_version(db, "lstm_forecaster")
    registry = AIModelRegistry(
        model_name="lstm_forecaster",
        version=version,
        file_path=str(forecaster.model_path),
        samples_count=stats.get("samples", 0),
        epochs=epochs,
        final_loss=stats.get("final_loss", 0.0),
        metrics_json=json.dumps(stats),
        status="active",
        is_current=True,
    )
    await _set_current_model(db, "lstm_forecaster")
    db.add(registry)
    await db.commit()

    return {"status": "completed", "results": stats}


@router.post("/train/baseline")
async def train_behavioral_baseline(
    db: AsyncSession = Depends(get_db),
    current_user: dict = Depends(require_permission(PERM_MANAGE_AI)),
):
    """Build behavioral baselines from historical resource data."""
    resource_monitor = get_resource_monitor()
    history = resource_monitor.get_history(limit=360)

    baseline = get_behavioral_baseline()
    stats = await baseline.bulk_update_from_snapshots(history, db_session=db)

    return {"status": "completed", "results": stats}


@router.get("/models")
async def list_models(
    db: AsyncSession = Depends(get_db),
    current_user: dict = Depends(require_permission(PERM_VIEW_DASHBOARD)),
):
    """List all model versions from registry."""
    result = await db.execute(
        select(AIModelRegistry).order_by(desc(AIModelRegistry.created_at)).limit(100)
    )
    rows = result.scalars().all()
    return [
        {
            "id": r.id,
            "model_name": r.model_name,
            "version": r.version,
            "trained_at": r.trained_at.isoformat(),
            "samples_count": r.samples_count,
            "epochs": r.epochs,
            "final_loss": r.final_loss,
            "status": r.status,
            "is_current": r.is_current,
        }
        for r in rows
    ]


@router.post("/models/{model_name}/rollback/{version}")
async def rollback_model(
    model_name: str,
    version: int,
    db: AsyncSession = Depends(get_db),
    current_user: dict = Depends(require_permission(PERM_MANAGE_AI)),
):
    """Rollback a model to a previous version."""
    result = await db.execute(
        select(AIModelRegistry).where(
            AIModelRegistry.model_name == model_name,
            AIModelRegistry.version == version,
        )
    )
    target = result.scalar_one_or_none()
    if not target:
        raise HTTPException(status_code=404, detail=f"Model {model_name} version {version} not found")

    await _set_current_model(db, model_name)
    target.is_current = True
    target.status = "active"
    await db.commit()

    return {"status": "rolled_back", "model_name": model_name, "version": version}


@router.get("/status")
async def ai_status(
    current_user: dict = Depends(require_permission(PERM_VIEW_DASHBOARD)),
):
    """Comprehensive AI health status."""
    anomaly_detector = get_anomaly_detector()
    ifo = get_isolation_forest_detector()
    zs = get_zscore_detector()
    ensemble = get_ensemble_detector()
    baseline = get_behavioral_baseline()
    forecaster = get_threat_forecaster()

    drift = ensemble.get_drift_score()
    last_ensemble = ensemble.get_last_result()

    return {
        "detectors": {
            "autoencoder": {
                "initialized": anomaly_detector.initialized,
                "threshold": anomaly_detector.threshold,
                "has_model": anomaly_detector.model is not None,
            },
            "isolation_forest": {
                "initialized": ifo.initialized,
                "has_model": ifo._model is not None,
            },
            "zscore": {
                "initialized": zs.initialized,
                "has_baseline": zs._mean is not None,
                "sample_count": zs._count,
            },
        },
        "ensemble": {
            "last_score": last_ensemble.get("ensemble_score") if last_ensemble else None,
            "last_is_anomaly": last_ensemble.get("is_anomaly") if last_ensemble else None,
            "drift_score": drift,
        },
        "baseline": baseline.get_learning_progress(),
        "forecaster": {
            "initialized": forecaster.initialized,
            "has_model": forecaster.model is not None,
        },
    }


@router.get("/anomaly-events")
async def get_anomaly_events(
    limit: int = Query(50, ge=1, le=500),
    offset: int = Query(0, ge=0),
    detector_type: str | None = None,
    is_anomaly_only: bool = False,
    db: AsyncSession = Depends(get_db),
    current_user: dict = Depends(require_permission(PERM_VIEW_DASHBOARD)),
):
    """Query persisted anomaly events."""
    query = select(AnomalyEvent).order_by(desc(AnomalyEvent.timestamp)).limit(limit).offset(offset)
    if detector_type:
        query = query.where(AnomalyEvent.detector_type == detector_type)
    if is_anomaly_only:
        query = query.where(AnomalyEvent.is_anomaly == True)

    result = await db.execute(query)
    rows = result.scalars().all()
    return [
        {
            "id": r.id,
            "timestamp": r.timestamp.isoformat(),
            "detector_type": r.detector_type,
            "anomaly_score": r.anomaly_score,
            "threshold": r.threshold,
            "is_anomaly": r.is_anomaly,
            "explanation": r.explanation,
            "confidence": r.confidence,
            "detector_scores": json.loads(r.detector_scores_json) if r.detector_scores_json else {},
            "feature_attribution": json.loads(r.feature_attribution_json) if r.feature_attribution_json else {},
            "context": json.loads(r.context_json) if r.context_json else {},
        }
        for r in rows
    ]


@router.get("/predictions")
async def get_predictions(
    current_user: dict = Depends(require_permission(PERM_VIEW_DASHBOARD)),
):
    """Current resource predictions and forecast alerts."""
    forecaster = get_threat_forecaster()
    resource_monitor = get_resource_monitor()

    if not forecaster.initialized or forecaster.model is None:
        return {"predictions": [], "forecast_alerts": [], "message": "Forecaster not trained yet"}

    history = resource_monitor.get_history(limit=60)
    if len(history) < 30:
        return {"predictions": [], "forecast_alerts": [], "message": "Insufficient history data"}

    trend = await forecaster.predict_trend(history, steps=6)
    alerts = forecaster.check_forecast_alerts(trend)

    return {
        "predictions": trend,
        "forecast_alerts": alerts,
        "actual_recent": history[-6:] if len(history) >= 6 else history,
    }


@router.get("/baselines")
async def get_baselines(
    current_user: dict = Depends(require_permission(PERM_VIEW_DASHBOARD)),
):
    """All behavioral baselines grouped by metric."""
    baseline = get_behavioral_baseline()
    all_baselines = baseline.get_all_baselines()

    grouped = {}
    for b in all_baselines:
        metric = b["metric_name"]
        if metric not in grouped:
            grouped[metric] = []
        grouped[metric].append(b)

    return {
        "baselines": grouped,
        "progress": baseline.get_learning_progress(),
    }


@router.get("/drift")
async def get_drift(
    current_user: dict = Depends(require_permission(PERM_VIEW_DASHBOARD)),
):
    """Model drift scores."""
    ensemble = get_ensemble_detector()
    return {
        "drift_score": ensemble.get_drift_score(),
        "last_result": ensemble.get_last_result(),
    }


@router.get("/feedback-stats")
async def get_feedback_stats(
    db: AsyncSession = Depends(get_db),
    current_user: dict = Depends(require_permission(PERM_VIEW_DASHBOARD)),
):
    """TP/FP counts and accuracy by module."""
    result = await db.execute(
        select(
            Alert.module_source,
            Alert.feedback,
            func.count(Alert.id),
        )
        .where(Alert.feedback.isnot(None))
        .group_by(Alert.module_source, Alert.feedback)
    )
    rows = result.all()

    by_module = {}
    total_tp = 0
    total_fp = 0
    for module, feedback, count in rows:
        if module not in by_module:
            by_module[module] = {"true_positive": 0, "false_positive": 0}
        by_module[module][feedback] = count
        if feedback == "true_positive":
            total_tp += count
        elif feedback == "false_positive":
            total_fp += count

    total = total_tp + total_fp
    accuracy = total_tp / max(total, 1)

    return {
        "total_true_positive": total_tp,
        "total_false_positive": total_fp,
        "accuracy": round(accuracy, 3),
        "by_module": by_module,
    }


# --- Helper functions ---

async def _get_next_version(db: AsyncSession, model_name: str) -> int:
    """Get the next version number for a model."""
    result = await db.execute(
        select(func.max(AIModelRegistry.version)).where(
            AIModelRegistry.model_name == model_name
        )
    )
    current_max = result.scalar()
    return (current_max or 0) + 1


async def _set_current_model(db: AsyncSession, model_name: str) -> None:
    """Unset is_current for all versions of a model."""
    await db.execute(
        update(AIModelRegistry)
        .where(AIModelRegistry.model_name == model_name)
        .values(is_current=False)
    )
